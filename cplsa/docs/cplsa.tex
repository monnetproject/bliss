\documentclass[a4paper,10pt]{article}
%\documentclass[a4paper,10pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{dsfont}

\title{Coupled Probabilistic Latent Semantic Analysis}
\author{John McCrae}

\pdfinfo{%
  /Title    (Coupled Probabilistic Latent Semantic Analysis)
  /Author   (John McCrae)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}

\begin{document}
\maketitle

\section{Preliminaries}

Assume the observed variables are

\[
 X = ( w^l_{jn} ) ; w^l_{jn} \in \{ 1,\ldots V \}
\]

Where there are $J$ documents each of length $N^l_j$ and $l \in \{1,2\}$. The
latent variables are

\[
 Y = ( z^l_{jn} ) ; z^l_{jn} \in \{ 1,\ldots K \}
\]

\section{Model definition}

Assume we have the following model variables

\[
 \Theta = ( \phi^l_{wk}, \theta^l_{kj} )
\]

And that the model is given as 
\[
 p_\Theta(X,Y) = \prod_j^J\prod_l^2\prod_n^{N^l_j}
\phi^l_{w^l_{jn}z^l_{jn}}\theta^l_{z^l_{jn}j}
\]

And we have the marginal distribution as follows:
\[
 p_\Theta(X) = \prod_j^J\prod_l^2\prod_n^{N^l_j}\prod_k^K
\phi^l_{w^l_{jn}k}\theta^l_{kj}
\]

Following \cite{platt} we define the posterior condition as 

\[
 \pi_{jk}(w_j,z_j) = \frac{\sum_n^{N^2_j} \mathds{1}(z^1_{jn} = k)}{N^1_j} - 
  \frac{\sum_n^{N_j^2} \mathds{1}(z^2_{jn} = k)}{N^2_j}
\]
\[
  = \frac{N^1_{jk}}{N^1_j} - \frac{N^2_{jk}}{N^2_j}
\]

And that

\[
 \pi(X,Y) = \sum_j^J\sum_k^K \pi_{jk}(w_j,z_j)
\]

Hence, the problem is to find $\Theta^*$ for some $\epsilon$ such that

\[
 \Theta^* = \arg\max_\Theta p_\Theta(Y|X)
\]

Such that

\[
 E_\Theta[\pi(X,Y)] \leq \epsilon
\]

\section{Solution}

By \cite{ganchev}, this can be solved by an E-M procedure where

\[
 \textrm{\bf{E}:}~~~~ q^{t+1} = \arg\min_q KL(q(Y) || p_{\Theta^t} (Y | X))
\]
\[
 \textrm{\bf{M}:}~~~~ \Theta^{t+1} = \arg\max_\Theta
E_{q^{t+1}}[log_\Theta(p(X,Y))]
\]

Where KL is as usual:
\[
 KL(p(X)||q(X)) = \sum_X p(x) \log\left(\frac{p(x)}{q(x)}\right)
\]

And $q(Y)$ is constrained by
\[
 q(Y) : E_q[\pi(X,Y)] \leq \epsilon
\]

By Lagrangian duality, the problem of solving
\[
 \arg\min_q KL(q(Y) || p_{\Theta} (Y | X))
\]
Subject to
\[
 E_q[\pi(X,Y)] \leq \xi ; ||\xi|| < \epsilon
\]

Is equivalent to finding
\[
 q^*(z_j) = \frac{p_\Theta(z_j|X) \exp(-\lambda^*
\theta(X,Y))}{\zeta(\lambda^*)}
\]
Where
\[
 \lambda^* = \arg\max_{\lambda \geq 0} - log(\zeta(\lambda)) - \epsilon\lambda
\]
And
\[
 \zeta(\lambda) = \sum_Y p_\Theta(Y|X) \exp(-\lambda \pi(X,Y))
 \]
 \[
 = \sum_j^J\sum_l^2 p_\Theta(z^l_j|X) e^{-
\lambda \pi_j(X,z_j)}
\]
It follows by differentiation
\[
 -\frac{\zeta'(\lambda^*)}{\zeta(\lambda^*)} - \epsilon = 0
\]
And the solution to this can be found by Newtonian gradient descent

Given we have $\lambda^*$ then we can apply

\[
 q^*(z_j) \propto p_\Theta(z_j|X) exp(-\lambda^*\pi(X,Y))
\]

And hence we can use a Gibbs-like sampling

\[
 p_\Theta(z^{tl}_{jn} = k|X) \propto
\phi^{tl}_{x_{jn}k}\theta^{tl}_{kj}exp(-\lambda^*\pi_{jk}(w_j,z_j))
\]

Finally the maximization step is trivial

\[
 \phi^l_{wk} = \frac{N^l_{wk}}{N^l_k}
\]

\[
 \theta^l_{kj} = \frac{N^l_{kj}}{N^l_j}
\]

\section{Dirichlet assumption}

The Dirichlet assumption is that 

\[
 \theta^l_j \sim Dir(\alpha)
\]
\[
 \phi^l_k \sim Dir(\beta)
\]

The result of which is that the maximization step is generalized
to~\cite{porteous}

\[
 \theta^l_{kj} = \frac{N^l_{kj} + \alpha}{N^l_k + K\alpha}
\]
\[
 \phi^l_{wk} = \frac{N^l_{wk} + \beta}{N^l_k + V \beta}
\]


\section{Initialization}

\begin{enumerate}
 \item Sort words by frequency $\{w'_1,\ldots w'_V\}$
 \item Find $K-1$ values, $\kappa_i$, such that $\kappa_i < \kappa_{i+1}$ and
   $\sum_{j = \kappa_i}^{\kappa_{i+1}} w'_j \leq \frac{N}{K}$
 \item Initialize $z^l_{jn} = k$ where $w^l_{jn} = w'_i$ and $\kappa_k \leq i <
\kappa_{i+1}$
 \item Calculate initial $\Theta$
 
\section{Notes}

\cite{platt} used 500 iterations at $\alpha=1.1$, $\beta=1.01$.
 
\end{enumerate}






\begin{thebibliography}{9}

\bibitem{platt}
John C. Platt and Kristina Toutanova and Wen-tau Yih (2010).
Translingual Document Representations from Discriminative Projections.

\bibitem{ganchev}
Kuzman Ganchev and João Graça and Jennifer Gillenwater and Ben Taskar (2010).
Posterior Regularization for Structured Latent Variable Models.

\bibitem{porteous}
Ian Porteous and David Newman and Alexander Ihler and Arthur Asuncion
and Padhraic Smyth and Max Welling (2008).
Fast Collapsed Gibbs Sampling For Latent Dirichlet Allocation.

\end{thebibliography}

\end{document}
